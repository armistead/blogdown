x <- head(unitermtbl)
print(x)
uniplot <- subset(unitermtbl, count>1500)
uniplotR <- ggplot(uniplot, aes(reorder(unigram, -count), count))
uniplotR <- uniplotR + geom_bar(stat = "identity") +
xlab("unigram") +
ylab("frequency") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Unigram")
uniplotR
#Plot BiGram Frequency
#bitermtbl <- arrange(bitermtbl, count)
biplot <- transform(bitermtbl,bigram=reorder(bigram, count))
x <- head(bitermtbl)
print(x)
#biplot <- subset(bitermtbl, count>75)
# biplotorder <- reorder(biplot, count)
#uniplotR <- ggplot(uniplot, aes(reorder(unigram, -count), count))
biplotR <- ggplot(subset(biplot, count>75), aes(bigram, count))
#aes(reorder(biplot, -count), count))
biplotR <- biplotR + geom_bar(stat = "identity") +
xlab("bigram") +
ylab("frequency") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Bigram")
biplotR
#Plot TriGram Frequency
triplot <- transform(tritermtbl,trigram=reorder(trigram, count))
x <- head(triplot)
print(x)
#biplot <- subset(bitermtbl, count>75)
# biplotorder <- reorder(biplot, count)
#uniplotR <- ggplot(uniplot, aes(reorder(unigram, -count), count))
triplotR <- ggplot(subset(triplot, count>12), aes(trigram, count))
#aes(reorder(biplot, -count), count))
triplotR <- triplotR + geom_bar(stat = "identity") +
xlab("trigram") +
ylab("frequency") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Trigram")
triplotR
saveRDS(bitermtbl, "bigram.RData")
saveRDS(tritermtbl, "trigram.RData")
shiny::runApp('C:/Users/johnarmistead/DataSciCoursera/Capstone/nlp_model')
runApp('C:/Users/johnarmistead/DataSciCoursera/Capstone/nlp_model')
runApp('C:/Users/johnarmistead/DataSciCoursera/Capstone/nlp_model')
runApp('C:/Users/johnarmistead/DataSciCoursera/Capstone/nlp_model')
runApp('C:/Users/johnarmistead/DataSciCoursera/Capstone/nlp_model')
runApp('C:/Users/johnarmistead/DataSciCoursera/Capstone/nlp_model')
class(new_word)
runApp('C:/Users/johnarmistead/DataSciCoursera/Capstone/nlp_model')
knitr::opts_chunk$set(echo = TRUE)
set.seed(37)
library(tidyverse) ## because we love it
library(stringi) ## for getting word counts
library(NLP)
library(SnowballC) ## for cleansing later on
library(RWeka)
library(tm) ## for building a corpus
blogs <- readLines(con <- file("./final/en_US/en_US.blogs.txt"), encoding = "UTF-8", skipNul = TRUE)
close(con)
news <- readLines(con <- file("./final/en_US/en_US.news.txt"), encoding = "UTF-8", skipNul = TRUE)
close(con)
twitter <- readLines(con <- file("./final/en_US/en_US.twitter.txt"), encoding = "UTF-8", skipNul = TRUE)
close(con)
files <- Sys.glob("final/en_US/*.txt")
size_blogs <- file.info("./final/en_US/en_US.blogs.txt")$size / 1024^2 ## convert to MB
size_news <- file.info("./final/en_US/en_US.news.txt")$size / 1024^2
size_twitter <- file.info("./final/en_US/en_US.twitter.txt")$size / 1024^2
sizes <- rbind(size_blogs, size_news, size_twitter)
len_blogs <- length(blogs)
len_news <- length(news)
len_twitter <- length(twitter)
lens <- rbind(len_blogs, len_news, len_twitter)
wordct_blogs <- sum(stri_count_words(blogs))
wordct_news <- sum(stri_count_words(news))
wordct_twitter <- sum(stri_count_words(twitter))
wordct <- rbind(wordct_blogs, wordct_news, wordct_twitter)
filestats <- data.frame(sizes, lens, wordct)
filestats
## take a random tenth of each text file
sample_blogs <- sample(blogs, size=len_blogs*.1)
sample_news <- sample(news, size=len_news*.1)
sample_twitter <- sample(twitter, size=len_twitter*.1)
## the following sapply functions convert character vectors into whatever coding you provide. The i stands for internationalization though in this case we only want a latin ASCII framework.
sample_blogs <- sapply(sample_blogs,function(row) iconv(row, "utf-8", "ASCII", sub=""))
sample_news <- sapply(sample_news,function(row) iconv(row, "utf-8", "ASCII", sub=""))
sample_twitter <- sapply(sample_twitter,function(row) iconv(row, "utf-8", "ASCII", sub=""))
sample_blogs <- (sample_blogs[!is.na(sample_blogs)])
sample_news <- (sample_blogs[!is.na(sample_blogs)])
sample_twitter <- (sample_blogs[!is.na(sample_blogs)])
sample <- sample(paste(sample_blogs, sample_news, sample_twitter), size=10000, replace = TRUE)
## Be careful in this section not to pass a character vector through the "tm" functions or else they will throw an error.
corpus <- VCorpus(VectorSource(sample))
# corpus <- lapply(corpus[1:2], as.character)
options(mc.cores = 1)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(removePunctuation),preserve_intra_word_dashes=TRUE)
corpus <- tm_map(corpus, content_transformer(stripWhitespace))    ## eliminate white space
corpus <- tm_map(corpus, content_transformer(removeNumbers))
corpus <- tm_map(corpus, content_transformer(stemDocument))   ## extracts stems of each word in vector
corpus <- tm_map(corpus, content_transformer(removeWords), stopwords(kind="en")) ## remove curse words
corpus <- tm_map(corpus, content_transformer(PlainTextDocument)) ## convert to txt doc because of corpora mappings
# data <- tm_map(data, content_transformer(function(x) gsub('http\\s+\\s*','',x)))
####### corpus <- VCorpus(VectorSource(corpus), readerControl = list(language="english"))
saveRDS(corpus, file = "./finalCorpus.RData")
# finalCorpusMem <- readRDS("./finalCorpus.RData")
# finalCorpus <- data.frame(text=unlist(sapply(finalCorpusMem,`[`, "content")),stringsAsFactors = FALSE)
# finalCorpus <- tm_map(finalCorpus, (PlainTextDocument))
# data <- SimpleCorpus(VectorSource(data))
## showing some lines of the textcorpus
## for (i in 1:10){
##  print(textCorpus[[i]]$content)
##}
# data <- corpus %>%
#        tm_map(content_transformer(tolower)) %>%
#        tm_map(content_transformer(removeNumbers)) %>%
#        tm_map(content_transformer(stemDocument)) %>%
#        tm_map(content_transformer(removeWords(stopwords("english")))) %>%
#        tm_map(content_transformer(removePunctuation( preserve_intra_word_contractions=TRUE,preserve_intra_word_dashes=TRUE))) %>%
#        tm_map(content_transformer(stripWhitespace)) %>%
#        tm_map(content_transformer(PlainTextDocument))
#Tokenize Data
data <- DocumentTermMatrix(corpus)
uniGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
biGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
triGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
# uniGramMatrix <- removeSparseTerms(DocumentTermMatrix(data, control = list(tokenize = uniGramTokenizer)), .999)
# data <- VCorpus(VectorSource(data))
# data <- DocumentTermMatrix(data)
data <- removeSparseTerms(data,.99999)
freq <- sort(colSums(as.matrix(data)), decreasing=TRUE)
#wf <- data.frame(unigram=names(freq), freq=freq)
triGramMatrix <- as.matrix(removeSparseTerms(TermDocumentMatrix(corpus, control = list(tokenize = triGramTokenizer)), .9998))
# use rm(list = ls()) and rerun if you get an error. May need to tweak ".9998" down.
biGramMatrix <- as.matrix(removeSparseTerms(TermDocumentMatrix(corpus, control = list(tokenize = biGramTokenizer)), .9998))
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
set.seed(37)
library(tidyverse) ## because we love it
library(stringi) ## for getting word counts
library(NLP)
library(SnowballC) ## for cleansing later on
library(RWeka)
library(tm) ## for building a corpus
blogs <- readLines(con <- file("./final/en_US/en_US.blogs.txt"), encoding = "UTF-8", skipNul = TRUE)
close(con)
news <- readLines(con <- file("./final/en_US/en_US.news.txt"), encoding = "UTF-8", skipNul = TRUE)
close(con)
twitter <- readLines(con <- file("./final/en_US/en_US.twitter.txt"), encoding = "UTF-8", skipNul = TRUE)
close(con)
files <- Sys.glob("final/en_US/*.txt")
size_blogs <- file.info("./final/en_US/en_US.blogs.txt")$size / 1024^2 ## convert to MB
size_news <- file.info("./final/en_US/en_US.news.txt")$size / 1024^2
size_twitter <- file.info("./final/en_US/en_US.twitter.txt")$size / 1024^2
sizes <- rbind(size_blogs, size_news, size_twitter)
len_blogs <- length(blogs)
len_news <- length(news)
len_twitter <- length(twitter)
lens <- rbind(len_blogs, len_news, len_twitter)
wordct_blogs <- sum(stri_count_words(blogs))
wordct_news <- sum(stri_count_words(news))
wordct_twitter <- sum(stri_count_words(twitter))
wordct <- rbind(wordct_blogs, wordct_news, wordct_twitter)
filestats <- data.frame(sizes, lens, wordct)
filestats
## take a random tenth of each text file
sample_blogs <- sample(blogs, size=len_blogs*.1)
sample_news <- sample(news, size=len_news*.1)
sample_twitter <- sample(twitter, size=len_twitter*.1)
## the following sapply functions convert character vectors into whatever coding you provide. The i stands for internationalization though in this case we only want a latin ASCII framework.
sample_blogs <- sapply(sample_blogs,function(row) iconv(row, "utf-8", "ASCII", sub=""))
sample_news <- sapply(sample_news,function(row) iconv(row, "utf-8", "ASCII", sub=""))
sample_twitter <- sapply(sample_twitter,function(row) iconv(row, "utf-8", "ASCII", sub=""))
sample_blogs <- (sample_blogs[!is.na(sample_blogs)])
sample_news <- (sample_blogs[!is.na(sample_blogs)])
sample_twitter <- (sample_blogs[!is.na(sample_blogs)])
sample <- sample(paste(sample_blogs, sample_news, sample_twitter), size=10000, replace = TRUE)
## Be careful in this section not to pass a character vector through the "tm" functions or else they will throw an error.
corpus <- VCorpus(VectorSource(sample))
# corpus <- lapply(corpus[1:2], as.character)
options(mc.cores = 1)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(removePunctuation),preserve_intra_word_dashes=TRUE)
corpus <- tm_map(corpus, content_transformer(stripWhitespace))    ## eliminate white space
corpus <- tm_map(corpus, content_transformer(removeNumbers))
corpus <- tm_map(corpus, content_transformer(stemDocument))   ## extracts stems of each word in vector
corpus <- tm_map(corpus, content_transformer(removeWords), stopwords(kind="en")) ## remove curse words
corpus <- tm_map(corpus, content_transformer(PlainTextDocument)) ## convert to txt doc because of corpora mappings
# data <- tm_map(data, content_transformer(function(x) gsub('http\\s+\\s*','',x)))
####### corpus <- VCorpus(VectorSource(corpus), readerControl = list(language="english"))
saveRDS(corpus, file = "./finalCorpus.RData")
# finalCorpusMem <- readRDS("./finalCorpus.RData")
# finalCorpus <- data.frame(text=unlist(sapply(finalCorpusMem,`[`, "content")),stringsAsFactors = FALSE)
# finalCorpus <- tm_map(finalCorpus, (PlainTextDocument))
# data <- SimpleCorpus(VectorSource(data))
## showing some lines of the textcorpus
## for (i in 1:10){
##  print(textCorpus[[i]]$content)
##}
# data <- corpus %>%
#        tm_map(content_transformer(tolower)) %>%
#        tm_map(content_transformer(removeNumbers)) %>%
#        tm_map(content_transformer(stemDocument)) %>%
#        tm_map(content_transformer(removeWords(stopwords("english")))) %>%
#        tm_map(content_transformer(removePunctuation( preserve_intra_word_contractions=TRUE,preserve_intra_word_dashes=TRUE))) %>%
#        tm_map(content_transformer(stripWhitespace)) %>%
#        tm_map(content_transformer(PlainTextDocument))
#Tokenize Data
data <- DocumentTermMatrix(corpus)
uniGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
biGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
triGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
# uniGramMatrix <- removeSparseTerms(DocumentTermMatrix(data, control = list(tokenize = uniGramTokenizer)), .999)
# data <- VCorpus(VectorSource(data))
# data <- DocumentTermMatrix(data)
data <- removeSparseTerms(data,.99999)
freq <- sort(colSums(as.matrix(data)), decreasing=TRUE)
#wf <- data.frame(unigram=names(freq), freq=freq)
triGramMatrix <- as.matrix(removeSparseTerms(TermDocumentMatrix(corpus, control = list(tokenize = triGramTokenizer)), .9998))
# use rm(list = ls()) and rerun if you get an error. May need to tweak ".9998" down.
biGramMatrix <- as.matrix(removeSparseTerms(TermDocumentMatrix(corpus, control = list(tokenize = biGramTokenizer)), .9998))
uniGramMatrix <- as.matrix(removeSparseTerms(TermDocumentMatrix(corpus), .9998))
trifreqterm <- as.data.frame(rowSums(triGramMatrix))
bifreqterm <- as.data.frame(rowSums(biGramMatrix))
unifreqterm <- as.data.frame(rowSums(uniGramMatrix))
trifreqterm$trigram <- row.names(trifreqterm)
bifreqterm$bigram <- row.names(bifreqterm)
unifreqterm$unigram <- row.names(unifreqterm)
tritermtbl <- as.tibble(data.frame(trifreqterm[,2],trifreqterm[,1]))
bitermtbl <- as.tibble(data.frame(bifreqterm[,2],bifreqterm[,1]))
unitermtbl <- as.tibble(data.frame(unifreqterm[,2],unifreqterm[,1]))
names(unitermtbl) <- c("unigram","count")
names(bitermtbl) <- c("bigram","count")
names(tritermtbl) <- c("trigram","count")
## Prep Unigram chart
unitermtbl <- arrange(unitermtbl, desc(count))
x <- head(unitermtbl)
print(x)
uniplot <- subset(unitermtbl, count>1500)
uniplotR <- ggplot(uniplot, aes(reorder(unigram, -count), count))
uniplotR <- uniplotR + geom_bar(stat = "identity") +
xlab("unigram") +
ylab("frequency") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Unigram")
uniplotR
#Plot BiGram Frequency
#bitermtbl <- arrange(bitermtbl, count)
biplot <- transform(bitermtbl,bigram=reorder(bigram, count))
x <- head(bitermtbl)
print(x)
#biplot <- subset(bitermtbl, count>75)
# biplotorder <- reorder(biplot, count)
#uniplotR <- ggplot(uniplot, aes(reorder(unigram, -count), count))
biplotR <- ggplot(subset(biplot, count>75), aes(bigram, count))
#aes(reorder(biplot, -count), count))
biplotR <- biplotR + geom_bar(stat = "identity") +
xlab("bigram") +
ylab("frequency") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Bigram")
biplotR
#Plot TriGram Frequency
triplot <- transform(tritermtbl,trigram=reorder(trigram, count))
x <- head(triplot)
print(x)
#biplot <- subset(bitermtbl, count>75)
# biplotorder <- reorder(biplot, count)
#uniplotR <- ggplot(uniplot, aes(reorder(unigram, -count), count))
triplotR <- ggplot(subset(triplot, count>12), aes(trigram, count))
#aes(reorder(biplot, -count), count))
triplotR <- triplotR + geom_bar(stat = "identity") +
xlab("trigram") +
ylab("frequency") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Trigram")
triplotR
saveRDS(bitermtbl, "./nlp_model/bigram.RData")
saveRDS(tritermtbl, "./nlp_model/trigram.RData")
runApp('C:/Users/johnarmistead/DataSciCoursera/Capstone/nlp_model')
runApp('C:/Users/johnarmistead/DataSciCoursera/Capstone/nlp_model')
runApp('C:/Users/johnarmistead/DataSciCoursera/Capstone/nlp_model')
runApp('C:/Users/johnarmistead/DataSciCoursera/Capstone/nlp_model')
saveRDS(biplot, "./nlp_model/bigram.RData")
saveRDS(triplot, "./nlp_model/trigram.RData")
runApp('C:/Users/johnarmistead/DataSciCoursera/Capstone/nlp_model')
runApp('C:/Users/johnarmistead/DataSciCoursera/Capstone/nlp_model')
bigramX <- biplot %>%
separate(col = bigram, c("bi_word_1", "bi_word_2"), sep=" ") %>%
arrange(desc(frequency)) %>%
as.data.frame()
bigramX <- biplot %>%
separate(col = bigram, c("bi_word_1", "bi_word_2"), sep=" ") %>%
arrange(desc(count)) %>%
as.data.frame()
trigramX <- triplot %>%
separate(col = trigram, c("tri_word_1", "tri_word_2", "tri_word_3"), sep=" ") %>%
arrange(desc(count)) %>%
as.data.frame()
saveRDS(bigramX, "./nlp_model/bigram.RData")
saveRDS(trigramX, "./nlp_model/trigram.RData")
runApp('C:/Users/johnarmistead/DataSciCoursera/Capstone/nlp_model')
runApp('C:/Users/johnarmistead/DataSciCoursera/Capstone/nlp_model')
runApp('C:/Users/johnarmistead/DataSciCoursera/Capstone/nlp_model')
runApp('C:/Users/johnarmistead/DataSciCoursera/Capstone/nlp_model')
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
set.seed(37)
library(tidyverse) ## because we love it
library(stringi) ## for getting word counts
library(NLP)
library(SnowballC) ## for cleansing later on
library(RWeka)
library(tm) ## for building a corpus
blogs <- readLines(con <- file("./final/en_US/en_US.blogs.txt"), encoding = "UTF-8", skipNul = TRUE)
close(con)
news <- readLines(con <- file("./final/en_US/en_US.news.txt"), encoding = "UTF-8", skipNul = TRUE)
close(con)
twitter <- readLines(con <- file("./final/en_US/en_US.twitter.txt"), encoding = "UTF-8", skipNul = TRUE)
close(con)
files <- Sys.glob("final/en_US/*.txt")
size_blogs <- file.info("./final/en_US/en_US.blogs.txt")$size / 1024^2 ## convert to MB
size_news <- file.info("./final/en_US/en_US.news.txt")$size / 1024^2
size_twitter <- file.info("./final/en_US/en_US.twitter.txt")$size / 1024^2
sizes <- rbind(size_blogs, size_news, size_twitter)
len_blogs <- length(blogs)
len_news <- length(news)
len_twitter <- length(twitter)
lens <- rbind(len_blogs, len_news, len_twitter)
wordct_blogs <- sum(stri_count_words(blogs))
wordct_news <- sum(stri_count_words(news))
wordct_twitter <- sum(stri_count_words(twitter))
wordct <- rbind(wordct_blogs, wordct_news, wordct_twitter)
filestats <- data.frame(sizes, lens, wordct)
filestats
## take a random tenth of each text file
sample_blogs <- sample(blogs, size=len_blogs*.2)
sample_news <- sample(news, size=len_news*.2)
sample_twitter <- sample(twitter, size=len_twitter*.2)
## the following sapply functions convert character vectors into whatever coding you provide. The i stands for internationalization though in this case we only want a latin ASCII framework.
sample_blogs <- sapply(sample_blogs,function(row) iconv(row, "utf-8", "ASCII", sub=""))
sample_news <- sapply(sample_news,function(row) iconv(row, "utf-8", "ASCII", sub=""))
sample_twitter <- sapply(sample_twitter,function(row) iconv(row, "utf-8", "ASCII", sub=""))
sample_blogs <- (sample_blogs[!is.na(sample_blogs)])
sample_news <- (sample_blogs[!is.na(sample_blogs)])
sample_twitter <- (sample_blogs[!is.na(sample_blogs)])
sample <- sample(paste(sample_blogs, sample_news, sample_twitter), size=20000, replace = TRUE)
## Be careful in this section not to pass a character vector through the "tm" functions or else they will throw an error.
corpus <- VCorpus(VectorSource(sample))
# corpus <- lapply(corpus[1:2], as.character)
options(mc.cores = 1)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(removePunctuation),preserve_intra_word_dashes=TRUE)
corpus <- tm_map(corpus, content_transformer(stripWhitespace))    ## eliminate white space
corpus <- tm_map(corpus, content_transformer(removeNumbers))
corpus <- tm_map(corpus, content_transformer(stemDocument))   ## extracts stems of each word in vector
corpus <- tm_map(corpus, content_transformer(removeWords), stopwords(kind="en")) ## remove curse words
corpus <- tm_map(corpus, content_transformer(PlainTextDocument)) ## convert to txt doc because of corpora mappings
# data <- tm_map(data, content_transformer(function(x) gsub('http\\s+\\s*','',x)))
####### corpus <- VCorpus(VectorSource(corpus), readerControl = list(language="english"))
saveRDS(corpus, file = "./finalCorpus.RData")
# finalCorpusMem <- readRDS("./finalCorpus.RData")
# finalCorpus <- data.frame(text=unlist(sapply(finalCorpusMem,`[`, "content")),stringsAsFactors = FALSE)
# finalCorpus <- tm_map(finalCorpus, (PlainTextDocument))
# data <- SimpleCorpus(VectorSource(data))
## showing some lines of the textcorpus
## for (i in 1:10){
##  print(textCorpus[[i]]$content)
##}
# data <- corpus %>%
#        tm_map(content_transformer(tolower)) %>%
#        tm_map(content_transformer(removeNumbers)) %>%
#        tm_map(content_transformer(stemDocument)) %>%
#        tm_map(content_transformer(removeWords(stopwords("english")))) %>%
#        tm_map(content_transformer(removePunctuation( preserve_intra_word_contractions=TRUE,preserve_intra_word_dashes=TRUE))) %>%
#        tm_map(content_transformer(stripWhitespace)) %>%
#        tm_map(content_transformer(PlainTextDocument))
#Tokenize Data
data <- DocumentTermMatrix(corpus)
uniGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
biGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
triGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
# uniGramMatrix <- removeSparseTerms(DocumentTermMatrix(data, control = list(tokenize = uniGramTokenizer)), .999)
# data <- VCorpus(VectorSource(data))
# data <- DocumentTermMatrix(data)
data <- removeSparseTerms(data,.99999)
freq <- sort(colSums(as.matrix(data)), decreasing=TRUE)
#Tokenize Data
data <- DocumentTermMatrix(corpus)
uniGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
biGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
triGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
# uniGramMatrix <- removeSparseTerms(DocumentTermMatrix(data, control = list(tokenize = uniGramTokenizer)), .999)
# data <- VCorpus(VectorSource(data))
# data <- DocumentTermMatrix(data)
data <- removeSparseTerms(data,.999)
freq <- sort(colSums(as.matrix(data)), decreasing=TRUE)
#wf <- data.frame(unigram=names(freq), freq=freq)
triGramMatrix <- as.matrix(removeSparseTerms(TermDocumentMatrix(corpus, control = list(tokenize = triGramTokenizer)), .999))
# use rm(list = ls()) and rerun if you get an error. May need to tweak ".9998" down.
biGramMatrix <- as.matrix(removeSparseTerms(TermDocumentMatrix(corpus, control = list(tokenize = biGramTokenizer)), .999))
uniGramMatrix <- as.matrix(removeSparseTerms(TermDocumentMatrix(corpus), .999))
trifreqterm <- as.data.frame(rowSums(triGramMatrix))
bifreqterm <- as.data.frame(rowSums(biGramMatrix))
unifreqterm <- as.data.frame(rowSums(uniGramMatrix))
trifreqterm$trigram <- row.names(trifreqterm)
bifreqterm$bigram <- row.names(bifreqterm)
unifreqterm$unigram <- row.names(unifreqterm)
tritermtbl <- as.tibble(data.frame(trifreqterm[,2],trifreqterm[,1]))
bitermtbl <- as.tibble(data.frame(bifreqterm[,2],bifreqterm[,1]))
unitermtbl <- as.tibble(data.frame(unifreqterm[,2],unifreqterm[,1]))
names(unitermtbl) <- c("unigram","count")
names(bitermtbl) <- c("bigram","count")
names(tritermtbl) <- c("trigram","count")
## Prep Unigram chart
unitermtbl <- arrange(unitermtbl, desc(count))
x <- head(unitermtbl)
print(x)
uniplot <- subset(unitermtbl, count>1500)
uniplotR <- ggplot(uniplot, aes(reorder(unigram, -count), count))
uniplotR <- uniplotR + geom_bar(stat = "identity") +
xlab("unigram") +
ylab("frequency") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Unigram")
uniplotR
#Plot BiGram Frequency
#bitermtbl <- arrange(bitermtbl, count)
biplot <- transform(bitermtbl,bigram=reorder(bigram, count))
x <- head(bitermtbl)
print(x)
#biplot <- subset(bitermtbl, count>75)
# biplotorder <- reorder(biplot, count)
#uniplotR <- ggplot(uniplot, aes(reorder(unigram, -count), count))
biplotR <- ggplot(subset(biplot, count>75), aes(bigram, count))
#aes(reorder(biplot, -count), count))
biplotR <- biplotR + geom_bar(stat = "identity") +
xlab("bigram") +
ylab("frequency") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Bigram")
biplotR
#Plot TriGram Frequency
triplot <- transform(tritermtbl,trigram=reorder(trigram, count))
x <- head(triplot)
print(x)
#biplot <- subset(bitermtbl, count>75)
# biplotorder <- reorder(biplot, count)
#uniplotR <- ggplot(uniplot, aes(reorder(unigram, -count), count))
triplotR <- ggplot(subset(triplot, count>12), aes(trigram, count))
#aes(reorder(biplot, -count), count))
triplotR <- triplotR + geom_bar(stat = "identity") +
xlab("trigram") +
ylab("frequency") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Trigram")
triplotR
bigramX <- biplot %>%
separate(col = bigram, c("bi_word_1", "bi_word_2"), sep=" ") %>%
arrange(desc(count)) %>%
as.data.frame()
trigramX <- triplot %>%
separate(col = trigram, c("tri_word_1", "tri_word_2", "tri_word_3"), sep=" ") %>%
arrange(desc(count)) %>%
as.data.frame()
saveRDS(bigramX, "./nlp_model/bigram.RData")
saveRDS(trigramX, "./nlp_model/trigram.RData")
runApp('C:/Users/johnarmistead/DataSciCoursera/Capstone/nlp_model')
runApp('C:/Users/johnarmistead/DataSciCoursera/Capstone/nlp_model')
install.packages("shinythemes")
runApp('C:/Users/johnarmistead/DataSciCoursera/Capstone/nlp_model')
runApp('C:/Users/johnarmistead/DataSciCoursera/Capstone/nlp_model')
library(shinythemes)
runApp('C:/Users/johnarmistead/DataSciCoursera/Capstone/nlp_model')
runApp('C:/Users/johnarmistead/DataSciCoursera/Capstone/nlp_model')
rm(list = ls())
install.packages(c("BH", "bindr", "bindrcpp", "blob", "broom", "callr", "caret", "CORElearn", "curl", "data.world", "DBI", "dbplyr", "ddalpha", "dwapi", "forcats", "forecast", "hms", "htmlwidgets", "httpuv", "klaR", "knitr", "lava", "leaflet", "lmtest", "lubridate", "network", "openssl", "packrat", "pgmm", "pillar", "plogr", "prodlim", "psych", "quanteda", "quantmod", "randomForest", "Rcpp", "RcppArmadillo", "RcppParallel", "readxl", "reticulate", "rlang", "rmarkdown", "robustbase", "rsconnect", "RSQLite", "Rttf2pt1", "selectr", "sfsmisc", "slam", "spacyr", "stringi", "stringr", "tidyselect", "timeDate", "tseries", "viridis", "withr", "XML", "xts", "yaml"))
install.packages("blogdown")
library(blogdown)
blogdown::install_hugo()
if(!require(installr)) {
install.packages("installr");
require(installr)
} #load / install+load installr
update.packages(ask = FALSE)
packs = as.data.frame(installed.packages(.libPaths()[1]), stringsAsFactors = F)
## and now re-install install packages using install.packages()
install.packages(packs$Package)
install.packages(packs$Package)
install.packages("yaml")
library(blogdown)
installed.packages("bea")
installed.packages("Zelig")
install.packages("blogdown")
library(blogdown)
library(blogdown)
setwd("C:/Users/johnarmistead/Dropbox/blogdown")
blogdown:::serve_site()
install_hugo()
install_hugo()
blogdown:::serve_site()
blogdown:::serve_site()
setwd("C:/Users/johnarmistead/Dropbox/blogdown")
blogdown:::serve_site()
blogdown:::serve_site()
install_theme("digitalcraftsman/hugo-strata-theme", theme_example=TRUE)
blogdown:::serve_site()
setwd("C:/Users/johnarmistead/Dropbox/blogdown")
blogdown:::serve_site()
blogdown::update_hugo()
blogdown:::update_meta_addin()
